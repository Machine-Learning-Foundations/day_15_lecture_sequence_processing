% no notes
\documentclass{beamer}
% notes and slides
%\documentclass[notes]{beamer}
% notes only
% \documentclass[notes=only]{beamer}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{multirow}
\usepackage{multimedia}
\usepackage{tikz}
\usepackage{circuitikz}
\usepackage{url}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}
%answer from Qrrbrbirlbel for https://tex.stackexchange.com/questions/134067/circuitikz-wire-kink-thingy-when-wires-cross
\tikzset{
  declare function={% in case of CVS which switches the arguments of atan2
    atan3(\a,\b)=ifthenelse(atan2(0,1)==90, atan2(\a,\b), atan2(\b,\a));},
  kinky cross radius/.initial=+.125cm,
  @kinky cross/.initial=+, kinky crosses/.is choice,
  kinky crosses/left/.style={@kinky cross=-},kinky crosses/right/.style={@kinky cross=+},
  kinky cross/.style args={(#1)--(#2)}{
    to path={
      let \p{@kc@}=($(\tikztotarget)-(\tikztostart)$),
          \n{@kc@}={atan3(\p{@kc@})+180} in
      -- ($(intersection of \tikztostart--{\tikztotarget} and #1--#2)!%
             \pgfkeysvalueof{/tikz/kinky cross radius}!(\tikztostart)$)
      arc [ radius     =\pgfkeysvalueof{/tikz/kinky cross radius},
            start angle=\n{@kc@},
            delta angle=\pgfkeysvalueof{/tikz/@kinky cross}180 ]
      -- (\tikztotarget)}}}

\usepackage{standalone}
\usepackage{adjustbox}
\usepackage{lmodern}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multimedia}
\usepackage{standalone}
\usepackage{csquotes}


\PassOptionsToPackage{american}{babel} % change this to your language(s), main language last
% Spanish languages need extra options in order to work with this template
% \PassOptionsToPackage{spanish,es-lcroman}{babel}
\usepackage{babel}

\PassOptionsToPackage{%
  backend=biber,bibencoding=utf8, %instead of bibtex
  %backend=bibtex8,bibencoding=ascii,%
  language=auto,%
  style=numeric-comp,%
  %style=authoryear-comp, % Author 1999, 2010
  %bibstyle=authoryear,dashed=false, % dashed: substitute rep. author with ---
  style=alphabetic,
  sorting=nyt, % name, year, title
  maxbibnames=10, % default: 3, et al.
  %backref=true,%
  %natbib=true % natbib compatibility mode (\citep and \citet still work)
}{biblatex}
\usepackage{biblatex}

\addbibresource{bib.bib}

\usetheme{metropolis}           % Use metropolis theme
\setbeamertemplate{caption}[default]
\title{Sequence Processing}
\date{\today}
\institute{Uni Bonn}
\author{Moritz Wolter}

\titlegraphic{\includegraphics[width=2.00cm]{UNI_Bonn_Logo_Standard_RZ.pdf}}
\begin{document}
    \maketitle

    \begin{frame}
    \frametitle{Overview} 
    \tableofcontents

    \end{frame}

    \begin{frame}{Motivation}
      \begin{itemize}
        \item Thus far we have never integrated information over time.
        \item We want the ability to create internal memory.
        \item Consider the sentence: I live in Paris. I speak ...
        \item ... French.
        \item Clearly it is likely for someone in Paris to speak French.
        \item Memory should help networks taking Paris into account when deciding what language is spoken.
      \end{itemize}
    \end{frame}

    \section{Recurrent neural networks}
    \begin{frame}{Elman-recurrent neural networks}
    A simple solution is to add a state to the network and feed this state recurrently back into the network \cite{elman1990finding},
    \begin{align}
        \overline{\mathbf{h}_t} &= \mathbf{W}_h \mathbf{h}_t 
            + \mathbf{W}_x \mathbf{x}_t + \mathbf{b}, \label{eq:simple_rnn} \\
        \mathbf{h}_{t+1} &= f(\overline{\mathbf{h}_t}).
    \end{align}
    \end{frame}

    \begin{frame}{Elman-recurrent neural networks}
      \begin{figure}
        \includestandalone{./figures/recurrentCell}
      \end{figure}
    \end{frame}

    \begin{frame}{Unrolling in Time}
      \begin{figure}
        \centering
        \includestandalone{./figures/unroll_recurrent_cell}
        \caption{The rolled (left) cell can be unrolled (right) by considering all inputs it saw
        during the current gradient computation iteration.}
        \label{fig:unroll_recurrent_cell}
    \end{figure}
    \end{frame}

    \begin{frame}{Stability of recurrent connections}
      For an intuition. Consider a linear network without activations or inputs.
      \begin{align}
        \mathbf{h}_{t+1} = \mathbf{W}_h \mathbf{h}_t
      \end{align}
      The evolution of the $\mathbf{h}$-sequence is guided by it's largest eigenvalue.
      If an eigenvalue larger than one exists. The state explodes.
      If all eigenvalues are smaller than one the state vanishes
      \cite{goodfellow2016deep}.
    \end{frame}

    \begin{frame}{Long Short Term Memory (LSTM)}
      \begin{figure}
        \includestandalone[width=\linewidth]{./figures/nop_lstm}
        \caption{An LSTM cell as described in\cite{hochreiter1997long,greff2016lstm}.}
      \end{figure}

    \end{frame}

    \begin{frame}{Long Short Term Memory (LSTM)}
      Llike a differentiable memory chip \cite{graves2012supervised} LSTM-memory can store $n_h$ numbers. Gates govern all changes to the cell state.
      Gate and state equations are defined as~\cite{hochreiter1997long,greff2016lstm}
      \begin{align}
          \overline{\mathbf{z}_t} &= \mathbf{W}_z \mathbf{x}_t + \mathbf{R}_z \mathbf{h}_{t-1}
                                + \mathbf{b}_z, \label{eq:state_candidate}
          \mathbf{z}_t = \tanh(\overline{\mathbf{z}_t}), \\
          \overline{\mathbf{i}_t} &= \mathbf{W}_i \mathbf{x}_t + \mathbf{R}_i \mathbf{h}_{t-1}
                               + \mathbf{p}_i \odot \mathbf{c}_{t-1}+ \mathbf{b}_i, \label{eq:input}
          \mathbf{i}_t = \sigma(\overline{\mathbf{i}_t}), \\
          \overline{\mathbf{f}_t} &= \mathbf{W}_f \mathbf{x}_t + \mathbf{R}_f \mathbf{h}_{t-1}
                                + \mathbf{p}_f \odot \mathbf{c}_{t-1}+ \mathbf{b}_f, \label{eq:forget} 
          \mathbf{f}_t = \sigma(\overline{\mathbf{f}_t}), \\
          \mathbf{c}_t &= \mathbf{z}_t \odot \mathbf{i}_t + \mathbf{c}_{t-1} \odot \mathbf{f}_t, \\
          \overline{\mathbf{o}_t} &= \mathbf{W}_o \mathbf{x}_t + \mathbf{R}_o \mathbf{h}_{t-1}
                                + \mathbf{p}_o \odot \mathbf{c}_t+ \mathbf{b}_o, \label{eq:output} 
          \mathbf{o}_t = \sigma(\overline{\mathbf{o}_t}), \\
          \mathbf{h}_t &= \tanh(\mathbf{c}_t) \odot \mathbf{o}_t.
      \end{align}
      Potential new states $\mathbf{z}_t$ are called block input. 
      $\mathbf{i}$ is called the input gate. The forget gate is $\mathbf{f}$ and
      $\mathbf{o}$ denotes the output gate.
      $\mathbf{p} \in \mathbb{R}^{n_h}$ are peephole weights,
      $\mathbf{W} \in \mathbb{R}^{n_i \times n_h}$ denotes input,
      $\mathbf{R} \in \mathbb{R}^{n_o \times n_h}$ are the recurrent matrices.
      $\odot$ indicates element-wise products. 
    \end{frame}


    \begin{frame}{Long Short Term Memory (LSTM)}
      \begin{figure}
        \includestandalone[width=.6\linewidth]{./figures/lstm}
        \caption{An LSTM-cell with peephole connections as described in \cite{hochreiter1997long,greff2016lstm}}
      \end{figure}
    \end{frame}


    \begin{frame}{Gated recurrent units}
      \begin{figure}
      \includestandalone[width=.6\linewidth]{./figures/gru}
      \end{figure}
    \end{frame}

    \begin{frame}{Orthogonal networks}
      TODO
    \end{frame}

    \begin{frame}{Summary}
      TODO
    \end{frame}

    \section{Applications}
    \begin{frame}{Language Processing}
      TODO
    \end{frame}

    \begin{frame}{Speech Processing}
      TODO
    \end{frame}

    \begin{frame}{Time-series forecasting}
      TODO
    \end{frame}

    %\section{Attention and Transformers}
    %\begin{frame}{Attention}
    %  TODO
    %\end{frame}
    %\begin{frame}{Transformers}
    %  TODO
    %\end{frame}

    \begin{frame}{Conclusion}
      TODO
    \end{frame}

    \begin{frame}{Literature}
      \printbibliography
    \end{frame}

\end{document}
